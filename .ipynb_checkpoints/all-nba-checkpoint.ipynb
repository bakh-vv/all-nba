{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from tensorflow import keras\n",
    "K = keras.backend\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd()\n",
    "csv_path = os.path.join(data_path, 'guard_data.csv')\n",
    "\n",
    "guard_data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Player</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Year End</th>\n",
       "      <th>Rk</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Pos_Rev</th>\n",
       "      <th>All NBA TEAM Yes / No</th>\n",
       "      <th>All NBA Team</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>ORB_PerGame</th>\n",
       "      <th>DRB_PerGame</th>\n",
       "      <th>TRB_PerGame</th>\n",
       "      <th>AST_PerGame</th>\n",
       "      <th>STL_PerGame</th>\n",
       "      <th>BLK_PerGame</th>\n",
       "      <th>TOV_PerGame</th>\n",
       "      <th>PF_PerGame</th>\n",
       "      <th>PS/G_PerGame</th>\n",
       "      <th>Which Team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Allen Leavell</td>\n",
       "      <td>HOU</td>\n",
       "      <td>1989</td>\n",
       "      <td>179</td>\n",
       "      <td>SG</td>\n",
       "      <td>SG</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alvin Robertson</td>\n",
       "      <td>SAS</td>\n",
       "      <td>1989</td>\n",
       "      <td>257</td>\n",
       "      <td>SG</td>\n",
       "      <td>SG</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Andre Turner</td>\n",
       "      <td>MIL</td>\n",
       "      <td>1989</td>\n",
       "      <td>316</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Anthony Bowie</td>\n",
       "      <td>SAS</td>\n",
       "      <td>1989</td>\n",
       "      <td>29</td>\n",
       "      <td>SG</td>\n",
       "      <td>SG</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Anthony Jones</td>\n",
       "      <td>TOT</td>\n",
       "      <td>1989</td>\n",
       "      <td>157</td>\n",
       "      <td>SG</td>\n",
       "      <td>SG</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID           Player   Tm  Year End   Rk Pos Pos_Rev All NBA TEAM Yes / No  \\\n",
       "0   1    Allen Leavell  HOU      1989  179  SG      SG                    No   \n",
       "1   2  Alvin Robertson  SAS      1989  257  SG      SG                    No   \n",
       "2   3     Andre Turner  MIL      1989  316  PG      PG                    No   \n",
       "3   4    Anthony Bowie  SAS      1989   29  SG      SG                    No   \n",
       "4   5    Anthony Jones  TOT      1989  157  SG      SG                    No   \n",
       "\n",
       "  All NBA Team  Age  ...  ORB_PerGame  DRB_PerGame  TRB_PerGame  AST_PerGame  \\\n",
       "0          NaN   31  ...          0.2          0.7          1.0          2.3   \n",
       "1          NaN   26  ...          2.4          3.5          5.9          6.0   \n",
       "2          NaN   24  ...          0.0          0.8          0.8          0.0   \n",
       "3          NaN   25  ...          1.4          1.7          3.1          1.6   \n",
       "4          NaN   26  ...          0.4          0.4          0.8          0.5   \n",
       "\n",
       "   STL_PerGame  BLK_PerGame  TOV_PerGame  PF_PerGame  PS/G_PerGame  Which Team  \n",
       "0          0.5          0.1          1.1         1.1           3.3           0  \n",
       "1          3.0          0.6          3.6         4.0          17.3           0  \n",
       "2          0.5          0.0          1.0         0.5           1.5           0  \n",
       "3          1.0          0.2          1.2         2.4           8.6           0  \n",
       "4          0.3          0.1          0.2         0.6           2.3           0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guard_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_guard_data = guard_data[['Year End', 'PS/G_PerGame', 'AST_PerGame', 'ORB_PerGame', 'DRB_PerGame', 'STL_PerGame', 'BLK_PerGame', 'TOV_PerGame', 'G', '3P%_Total', '2P%_Total', 'TS%_ADV', 'USG%_ADV', 'OWS_ADV', 'DWS_ADV', 'OBPM_ADV', 'DBPM_ADV', 'VORP_ADV', 'ORtg_Per100', 'DRtg_Per100']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year End</th>\n",
       "      <th>PS/G_PerGame</th>\n",
       "      <th>AST_PerGame</th>\n",
       "      <th>ORB_PerGame</th>\n",
       "      <th>DRB_PerGame</th>\n",
       "      <th>STL_PerGame</th>\n",
       "      <th>BLK_PerGame</th>\n",
       "      <th>TOV_PerGame</th>\n",
       "      <th>G</th>\n",
       "      <th>3P%_Total</th>\n",
       "      <th>2P%_Total</th>\n",
       "      <th>TS%_ADV</th>\n",
       "      <th>USG%_ADV</th>\n",
       "      <th>OWS_ADV</th>\n",
       "      <th>DWS_ADV</th>\n",
       "      <th>OBPM_ADV</th>\n",
       "      <th>DBPM_ADV</th>\n",
       "      <th>VORP_ADV</th>\n",
       "      <th>ORtg_Per100</th>\n",
       "      <th>DRtg_Per100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1989</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>55</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.417</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>123.0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1989</td>\n",
       "      <td>17.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>65</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.523</td>\n",
       "      <td>22.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>107.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1989</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>31.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1989</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>18</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.515</td>\n",
       "      <td>15.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>96.0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1989</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>33</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.442</td>\n",
       "      <td>20.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year End  PS/G_PerGame  AST_PerGame  ORB_PerGame  DRB_PerGame  STL_PerGame  \\\n",
       "0      1989           3.3          2.3          0.2          0.7          0.5   \n",
       "1      1989          17.3          6.0          2.4          3.5          3.0   \n",
       "2      1989           1.5          0.0          0.0          0.8          0.5   \n",
       "3      1989           8.6          1.6          1.4          1.7          1.0   \n",
       "4      1989           2.3          0.5          0.4          0.4          0.3   \n",
       "\n",
       "   BLK_PerGame  TOV_PerGame   G  3P%_Total  2P%_Total  TS%_ADV  USG%_ADV  \\\n",
       "0          0.1          1.1  55      0.122      0.408    0.417      17.7   \n",
       "1          0.6          3.6  65      0.200      0.497    0.523      22.2   \n",
       "2          0.0          1.0   4        NaN      0.500    0.500      31.9   \n",
       "3          0.2          1.2  18      0.200      0.511    0.515      15.3   \n",
       "4          0.1          0.2  33      0.250      0.397    0.442      20.1   \n",
       "\n",
       "   OWS_ADV  DWS_ADV  OBPM_ADV  DBPM_ADV  VORP_ADV  ORtg_Per100  DRtg_Per100  \n",
       "0      0.0      1.2      -2.3       1.8       0.3        123.0          104  \n",
       "1      0.5      0.4      -0.9      -2.0      -0.1        107.0          106  \n",
       "2      1.1      0.8       0.1      -2.4      -0.1         89.0          106  \n",
       "3     -0.4      1.4      -3.2       0.2      -0.3         96.0           91  \n",
       "4      0.4      0.4      -1.6      -0.9      -0.1        110.0          109  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_guard_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bakhtiyar\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_guard_data)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='median', verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.fit(X_guard_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4789, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_guard_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "defs = np.append(imputer.statistics_, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0912 07:47:21.667040  9828 deprecation.py:323] From C:\\Users\\Bakhtiyar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0912 07:47:21.676036  9828 deprecation.py:323] From C:\\Users\\Bakhtiyar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\readers.py:498: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "W0912 07:47:21.736999  9828 deprecation.py:323] From C:\\Users\\Bakhtiyar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\readers.py:211: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "csv_path = os.path.join(data_path, 'guard_data.csv')\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(csv_path, batch_size = my_batch_size,\n",
    "                                               select_columns = ['Year End', 'PS/G_PerGame', 'AST_PerGame', 'ORB_PerGame', 'DRB_PerGame', 'STL_PerGame', 'BLK_PerGame', 'TOV_PerGame', 'G', '3P%_Total', '2P%_Total', 'TS%_ADV', 'USG%_ADV', 'OWS_ADV', 'DWS_ADV', 'OBPM_ADV', 'DBPM_ADV', 'VORP_ADV', 'ORtg_Per100', 'DRtg_Per100', 'Which Team'],\n",
    "                                               column_defaults = defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_size = 4789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * full_dataset_size)\n",
    "valid_size = int(0.15 * full_dataset_size)\n",
    "test_size = int(0.15 * full_dataset_size)\n",
    "\n",
    "#full_dataset = tf.data.TFRecordDataset(FLAGS.input_file)\n",
    "dataset = dataset.shuffle(500)\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "valid_dataset = test_dataset.skip(valid_size)\n",
    "test_dataset = test_dataset.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess(data):\n",
    "    #x = list(data.values())[:-1]\n",
    "    #y = [data['Which Team']]\n",
    "    #x = tf.stack(x)\n",
    "    #y = tf.stack(y)\n",
    "    x = tf.transpose(tf.stack(list(data.values())[:-1]))\n",
    "    #x = tf.stack(list(data.values())[:-1]).transpose()\n",
    "    y = tf.transpose((tf.stack([data['Which Team']])))\n",
    "    return (x - X_mean) / X_std, y\n",
    "    #return (x - np.array([X_mean] * my_batch_size).T) / np.array([X_std] * my_batch_size).T, y\n",
    "    #return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = valid_dataset.map(preprocess).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((32, 20), (32, 1)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[ 1.22907236e+00  8.50121132e+00 -1.13640495e+00 -1.26177240e-01\n",
      "  -1.15194194e+00  4.62633247e+01 -2.69145297e+00 -1.41801866e+00\n",
      "  -2.39027402e+00 -4.09120135e+00 -7.36889392e+00  1.03041126e+03\n",
      "   1.83365656e+01 -3.54481043e-01  1.34567745e+00  1.71416788e+00\n",
      "   8.01775912e-01 -2.69065522e-01 -6.12023117e+00 -1.94814575e+01]\n",
      " [-1.39452990e-01  7.83673359e+00 -1.16947219e+00 -1.31023361e-01\n",
      "  -1.18844344e+00  3.71507699e+01 -4.98984246e+00  6.96316256e-01\n",
      "   2.91755546e-01  1.94784406e-01  9.98860507e-02  1.27771713e+03\n",
      "   1.77336912e+01 -2.05104931e-01  5.53635504e-01  8.78629009e-01\n",
      "   4.93420722e-01 -3.46526623e-01 -6.18854329e+00 -1.98944586e+01]\n",
      " [-5.12687175e-01  6.67389757e+00 -1.12185537e+00 -1.84330689e-01\n",
      "  -1.19218719e+00  3.20249579e+01  7.56131269e-01 -1.18309256e+00\n",
      "  -2.20613467e+00 -2.33901991e+01 -7.36889392e+00  1.30127007e+03\n",
      "   1.75327331e+01 -3.54481043e-01  1.34567745e+00  1.52549781e+00\n",
      "   1.02202962e+00 -2.69065522e-01 -6.13886175e+00 -1.94608074e+01]\n",
      " [-1.13474415e+00  1.21558388e+01 -1.29204144e+00 -2.44087032e-02\n",
      "  -1.16036536e+00  2.63296112e+01  1.33972735e+01  6.96316256e-01\n",
      "  -2.17010741e+00 -8.91595080e+00  2.95660834e+00  1.14817596e+03\n",
      "   1.83365656e+01 -4.54065117e-01 -2.38406446e-01  1.06729908e+00\n",
      "   5.37471464e-01 -4.23987724e-01 -6.19475348e+00 -2.11747620e+01]\n",
      " [-7.61509965e-01  5.34494213e+00 -1.13728675e+00 -1.35869481e-01\n",
      "  -1.16036536e+00  3.05061988e+01  1.79940525e+01  3.43927103e-01\n",
      "  -2.14608923e+00 -3.28707644e+00  1.09875545e+01  1.23241669e-01\n",
      "   1.57241101e+01 -4.54065117e-01 -4.76019031e-01  7.43864675e-01\n",
      "   4.49369981e-01 -4.23987724e-01 -6.21338406e+00 -2.10508617e+01]\n",
      " [ 2.33781195e-01  7.67061416e+00 -1.12273716e+00 -2.13407414e-01\n",
      "  -1.18095595e+00  2.91772846e+01 -9.67660851e-01 -1.65294476e+00\n",
      "  -2.56640730e+00 -5.39469456e+01 -6.22161589e+00  1.33659948e+03\n",
      "   1.73317750e+01 -2.05104931e-01  8.70452284e-01  9.32534743e-01\n",
      "   5.37471464e-01 -3.46526623e-01 -6.18854329e+00 -2.03281098e+01]\n",
      " [ 1.47789515e+00  6.34165871e+00 -1.12890971e+00 -2.78830045e-01\n",
      "  -1.17627627e+00  3.94289086e+01  1.90532602e+00 -1.30055561e+00\n",
      "   2.91755546e-01  1.94784406e-01  9.98860507e-02  1.23061125e+03\n",
      "   1.87384818e+01 -4.04273080e-01  2.36818724e-01  1.04034621e+00\n",
      "   4.93420722e-01 -3.46526623e-01 -6.17612291e+00 -2.00183589e+01]\n",
      " [ 4.82603985e-01  8.33509188e+00 -1.12141447e+00 -2.57022501e-01\n",
      "  -1.16504504e+00  2.67093010e+01 -1.54225822e+00 -8.30703405e-01\n",
      "  -2.30220737e+00 -2.48295154e+00 -6.22161589e+00  1.12462302e+03\n",
      "   1.91403981e+01 -4.54065117e-01  7.84103338e-02  7.43864675e-01\n",
      "   4.49369981e-01 -4.23987724e-01 -6.20717387e+00 -2.08650112e+01]\n",
      " [ 1.35348375e+00  8.59717486e-01 -1.19416240e+00  3.26935054e-01\n",
      "  -1.33257758e+00  1.94951952e+01 -3.93063478e-01 -7.13240354e-01\n",
      "  -2.27418617e+00  4.75417263e+00 -3.92705983e+00  1.08929361e+03\n",
      "   1.61260263e+01 -4.54065117e-01 -2.38406446e-01  5.28241741e-01\n",
      "   4.05319239e-01 -4.23987724e-01 -6.24443503e+00 -2.15877631e+01]\n",
      " [-2.63864385e-01  1.21558388e+01 -1.12626433e+00 -2.37638018e-01\n",
      "  -1.19967467e+00  4.22765820e+01 -3.93063478e-01 -1.18309256e+00\n",
      "  -2.26618011e+00 -3.28707644e+00 -5.07433786e+00  1.28949360e+03\n",
      "   1.59250682e+01 -2.05104931e-01  6.32839699e-01  1.66026215e+00\n",
      "   9.33928137e-01 -2.69065522e-01 -6.12644136e+00 -1.83663545e+01]\n",
      " [-7.61509965e-01  9.83016676e+00 -1.10421950e+00 -1.31023361e-01\n",
      "  -1.15100600e+00  4.11375126e+01 -9.67660851e-01 -1.53548171e+00\n",
      "  -2.32222252e+00 -4.89532626e+00 -6.22161589e+00  1.24238772e+03\n",
      "   1.77336912e+01 -3.04689006e-01  1.02886067e+00  9.32534743e-01\n",
      "   6.25572947e-01 -2.69065522e-01 -6.16991271e+00 -1.88619559e+01]\n",
      " [-3.88275780e-01  1.85643407e+00 -1.15404081e+00  2.16294443e-02\n",
      "  -1.12760760e+00  3.29741824e+01  2.47992339e+00 -1.41801866e+00\n",
      "  -2.05001653e+00 -2.25860742e+01 -2.77978180e+00  1.23241669e-01\n",
      "   1.57241101e+01 -2.54896968e-01 -3.17610641e-01  9.32534743e-01\n",
      "   6.25572947e-01 -4.23987724e-01 -6.18233310e+00 -2.06791607e+01]\n",
      " [ 1.22907236e+00  6.84001701e+00 -1.15624530e+00 -2.13407414e-01\n",
      "  -1.22588088e+00  4.58836349e+01  2.25908315e+01 -1.25925100e-01\n",
      "  -2.12607409e+00 -3.28707644e+00  2.01657788e+01  1.40725830e+03\n",
      "   1.79346494e+01 -4.54065117e-01  7.84103338e-02  1.12120481e+00\n",
      "   4.49369981e-01 -3.46526623e-01 -6.15128213e+00 -1.98944586e+01]\n",
      " [-1.25915555e+00  1.13252416e+01 -1.12802792e+00 -7.04468507e-02\n",
      "  -1.14071071e+00  4.83516185e+01 -9.67660851e-01 -1.53548171e+00\n",
      "  -2.31821949e+00 -1.05242006e+01 -6.22161589e+00  1.28949360e+03\n",
      "   1.67289007e+01 -2.05104931e-01  3.95227114e-01  1.25596914e+00\n",
      "   8.01775912e-01 -3.46526623e-01 -6.12644136e+00 -1.81598540e+01]\n",
      " [-1.75680113e+00  8.83345018e+00 -1.14125481e+00 -4.37931864e-02\n",
      "  -1.12292792e+00  3.44929415e+01  2.94859999e+01  1.16616846e+00\n",
      "  -2.08604380e+00 -1.21324504e+01  2.01657788e+01  1.36015242e+03\n",
      "   1.85375237e+01 -2.54896968e-01  6.32839699e-01  1.28292201e+00\n",
      "   6.69623688e-01 -2.69065522e-01 -6.16370252e+00 -1.94195073e+01]\n",
      " [-1.38356694e+00  4.84658383e+00 -1.19416240e+00 -3.15175951e-01\n",
      "  -1.27829329e+00  4.28461166e+01 -9.67660851e-01 -1.41801866e+00\n",
      "  -2.25417102e+00 -1.21324504e+01 -6.22161589e+00  1.12462302e+03\n",
      "   1.77336912e+01 -4.54065117e-01 -4.76019031e-01  6.63006075e-01\n",
      "   3.61268498e-01 -3.46526623e-01 -6.21959426e+00 -2.12160621e+01]\n",
      " [-1.50797834e+00  8.66733075e+00 -1.13596406e+00 -4.37931864e-02\n",
      "  -1.14913413e+00  4.62633247e+01  5.92750763e+00 -1.25925100e-01\n",
      "  -2.16210135e+00 -1.29365753e+01  1.80933031e+00  1.30127007e+03\n",
      "   1.81356075e+01 -2.05104931e-01 -1.59202251e-01  8.51676142e-01\n",
      "   5.37471464e-01 -4.23987724e-01 -6.17612291e+00 -1.98118584e+01]\n",
      " [-7.61509965e-01  9.66404733e+00 -1.11656461e+00 -3.17599011e-01\n",
      "  -1.18376376e+00  3.94289086e+01 -3.93063478e-01 -1.53548171e+00\n",
      "  -2.29420131e+00 -3.54520727e+01 -9.66344998e+00  1.13639949e+03\n",
      "   1.83365656e+01 -4.54065117e-01 -7.93861215e-04  1.20206341e+00\n",
      "   5.81522205e-01 -4.23987724e-01 -6.17612291e+00 -2.05552604e+01]\n",
      " [-1.13474415e+00  1.13252416e+01 -1.21841171e+00 -1.35869481e-01\n",
      "  -1.19873874e+00  2.76585254e+01  6.50210500e+00 -5.95777303e-01\n",
      "  -2.19012256e+00 -1.85654497e+01 -5.07433786e+00  1.14817596e+03\n",
      "   1.79346494e+01 -1.05520857e-01  1.50408584e+00  3.19657555e+00\n",
      "   9.77978878e-01 -4.23987724e-01 -6.12023117e+00 -1.96879581e+01]\n",
      " [ 1.60230654e+00 -1.36999101e-01 -1.23692937e+00 -2.90945347e-01\n",
      "  -1.32602603e+00  4.56937900e+01 -1.54225822e+00  5.78853205e-01\n",
      "  -2.26618011e+00  7.16654735e+00  6.62052285e-01  1.23241669e-01\n",
      "   1.57241101e+01 -6.03441228e-01 -2.38406446e-01  8.51676142e-01\n",
      "   4.05319239e-01 -3.46526623e-01 -6.18854329e+00 -2.14019126e+01]\n",
      " [-6.37098570e-01  8.83345018e+00 -1.12185537e+00 -1.72215387e-01\n",
      "  -1.18657157e+00  3.20249579e+01 -4.41524509e+00 -1.30055561e+00\n",
      "  -2.36625584e+00 -5.69945117e+00 -9.66344998e+00  9.47975974e+02\n",
      "   1.79346494e+01 -1.05520857e-01  1.02886067e+00  1.28292201e+00\n",
      "   8.01775912e-01 -1.91604420e-01 -6.15128213e+00 -1.97705583e+01]\n",
      " [-7.61509965e-01  1.21558388e+01 -1.14434109e+00 -1.21331119e-01\n",
      "  -1.16691691e+00  3.44929415e+01 -9.67660851e-01 -1.53548171e+00\n",
      "  -2.40228310e+00 -1.85654497e+01 -7.36889392e+00  1.27771713e+03\n",
      "   1.93413562e+01 -4.04273080e-01  4.74431309e-01  1.44463921e+00\n",
      "   5.37471464e-01 -3.46526623e-01 -6.15749233e+00 -2.04933102e+01]\n",
      " [ 1.60230654e+00  1.08268834e+01 -1.22193889e+00 -1.55253965e-01\n",
      "  -1.19218719e+00  5.84133976e+01  4.20371551e+00 -8.30703405e-01\n",
      "  -2.17811347e+00 -1.21324504e+01 -2.77978180e+00  1.37192889e+03\n",
      "   1.77336912e+01 -5.57288200e-02  1.42488165e+00  1.71416788e+00\n",
      "   7.13674430e-01  4.07788829e-02 -6.08918020e+00 -1.79946536e+01]\n",
      " [ 2.33781195e-01  1.24880777e+01 -1.12758702e+00 -2.49753320e-01\n",
      "  -1.15662162e+00  2.53803867e+01 -3.93063478e-01 -1.53548171e+00\n",
      "  -2.21814376e+00 -3.28707644e+00 -5.07433786e+00  1.20705831e+03\n",
      "   1.71308169e+01 -4.04273080e-01  3.16022919e-01  8.24723276e-01\n",
      "   4.05319239e-01 -3.46526623e-01 -6.20717387e+00 -2.05759104e+01]\n",
      " [-1.39452990e-01  9.49792790e+00 -1.15756799e+00 -3.32137374e-01\n",
      "  -1.26238238e+00  3.33538721e+01  9.94968924e+00  1.75348371e+00\n",
      "  -2.19412558e+00  9.57892208e+00  1.78712227e+01  9.59752444e+02\n",
      "   1.63269844e+01 -5.03857154e-01 -3.17610641e-01  7.70817542e-01\n",
      "   4.49369981e-01 -4.23987724e-01 -6.20717387e+00 -2.10095616e+01]\n",
      " [-7.61509965e-01  1.09930028e+01 -1.14169571e+00 -1.26177240e-01\n",
      "  -1.19312312e+00  3.24046477e+01 -3.93063478e-01  2.26464053e-01\n",
      "  -2.29019829e+00  1.53767300e+00 -5.07433786e+00  1.11284655e+03\n",
      "   1.85375237e+01 -4.04273080e-01  1.57614529e-01  1.63330928e+00\n",
      "   7.57725171e-01 -2.69065522e-01 -6.14507194e+00 -1.98531585e+01]\n",
      " [ 1.35348375e+00  6.67389757e+00 -1.08217467e+00 -1.52830904e-01\n",
      "  -1.13509509e+00  4.45547207e+01 -9.67660851e-01 -1.41801866e+00\n",
      "  -2.29019829e+00 -1.53489501e+01 -7.36889392e+00  1.23061125e+03\n",
      "   1.73317750e+01 -5.03857154e-01 -2.38406446e-01  8.24723276e-01\n",
      "   3.61268498e-01 -3.46526623e-01 -6.19475348e+00 -2.07617609e+01]\n",
      " [-8.85921360e-01  7.33837530e+00 -1.12009178e+00 -2.86099226e-01\n",
      "  -1.17253253e+00  3.50624761e+01  1.81533896e-01 -1.25925100e-01\n",
      "  -2.32222252e+00 -2.48295154e+00 -9.66344998e+00  1.24238772e+03\n",
      "   1.79346494e+01 -4.54065117e-01 -3.17610641e-01  7.43864675e-01\n",
      "   4.49369981e-01 -4.23987724e-01 -6.20096368e+00 -2.09889115e+01]\n",
      " [ 2.33781195e-01  1.21558388e+01 -1.14610468e+00  4.34369879e-02\n",
      "  -1.13322322e+00  2.27225583e+01  7.56131269e-01 -3.60851201e-01\n",
      "  -2.31021343e+00 -1.67882663e+00 -8.51617195e+00  1.06574067e+03\n",
      "   1.83365656e+01 -5.57288200e-02  2.69214877e+00  1.28292201e+00\n",
      "   9.77978878e-01  4.07788829e-02 -6.15128213e+00 -2.00596590e+01]\n",
      " [-1.13474415e+00  1.06607639e+01 -1.11965088e+00 -1.38292542e-01\n",
      "  -1.18937938e+00  2.19631787e+01 -3.26605034e+00 -8.30703405e-01\n",
      "  -2.39828007e+00 -8.11182589e+00 -1.31052841e+01  1.18350537e+03\n",
      "   1.59250682e+01 -2.54896968e-01  1.58329004e+00  1.90283795e+00\n",
      "   1.55063852e+00 -1.91604420e-01 -6.15128213e+00 -2.06791607e+01]\n",
      " [-1.13474415e+00  1.21558388e+01 -1.13860943e+00 -9.95235755e-02\n",
      "  -1.18095595e+00  3.27843375e+01  8.22589712e+00 -3.60851201e-01\n",
      "  -2.24616496e+00 -1.67882663e+00 -1.63250377e+00  1.23061125e+03\n",
      "   1.83365656e+01 -1.55312894e-01  1.02886067e+00  1.36378061e+00\n",
      "   9.77978878e-01 -1.91604420e-01 -6.15749233e+00 -1.98531585e+01]\n",
      " [-1.75680113e+00  1.14913611e+01 -1.15359992e+00  3.13216859e-02\n",
      "  -1.13883883e+00  2.93671294e+01  1.81533896e-01 -1.41801866e+00\n",
      "  -2.26217708e+00 -2.25860742e+01 -6.22161589e+00  1.14817596e+03\n",
      "   1.59250682e+01 -5.03857154e-01 -7.93861215e-04  1.30987488e+00\n",
      "   4.93420722e-01 -2.69065522e-01 -6.18233310e+00 -2.05965605e+01]], shape=(32, 20), dtype=float64)\n",
      "y = tf.Tensor(\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]], shape=(32, 1), dtype=float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in valid_dataset.take(1):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(preprocess).prefetch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create panda datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_guard_data =  guard_data[['Year End', 'PS/G_PerGame', 'AST_PerGame', 'ORB_PerGame', 'DRB_PerGame', 'STL_PerGame', 'BLK_PerGame', 'TOV_PerGame', 'G', '3P%_Total', '2P%_Total', 'TS%_ADV', 'USG%_ADV', 'OWS_ADV', 'DWS_ADV', 'OBPM_ADV', 'DBPM_ADV', 'VORP_ADV', 'ORtg_Per100', 'DRtg_Per100', 'Which Team']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 70%-15%-15%\n",
    "train_set, valid_set = np.split(selected_guard_data.sample(frac=1), [int(.7*len(selected_guard_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1437"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sets 70%-15%-15%, stratified with regards to column \"Which Team\"\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, remain_index in split.split(selected_guard_data, selected_guard_data[\"Which Team\"]):\n",
    "    train_set = selected_guard_data.iloc[train_index]\n",
    "    remain_set = selected_guard_data.iloc[remain_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3233\n",
       "3      40\n",
       "1      40\n",
       "2      39\n",
       "Name: Which Team, dtype: int64"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[\"Which Team\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "for valid_index, test_index in split.split(remain_set, remain_set[\"Which Team\"]):\n",
    "    valid_set = remain_set.iloc[valid_index]\n",
    "    test_set = remain_set.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_set.drop(columns = ['Which Team']), train_set['Which Team']\n",
    "X_valid, y_valid = valid_set.drop(columns = ['Which Team']), valid_set['Which Team']\n",
    "X_test, y_test = test_set.drop(columns = ['Which Team']), test_set['Which Team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X, columns=X_train.columns,\n",
    "                          index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(X_valid)\n",
    "X_valid = pd.DataFrame(X, columns=X_valid.columns,\n",
    "                          index=X_valid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year End</th>\n",
       "      <th>PS/G_PerGame</th>\n",
       "      <th>AST_PerGame</th>\n",
       "      <th>ORB_PerGame</th>\n",
       "      <th>DRB_PerGame</th>\n",
       "      <th>STL_PerGame</th>\n",
       "      <th>BLK_PerGame</th>\n",
       "      <th>TOV_PerGame</th>\n",
       "      <th>G</th>\n",
       "      <th>3P%_Total</th>\n",
       "      <th>2P%_Total</th>\n",
       "      <th>TS%_ADV</th>\n",
       "      <th>USG%_ADV</th>\n",
       "      <th>OWS_ADV</th>\n",
       "      <th>DWS_ADV</th>\n",
       "      <th>OBPM_ADV</th>\n",
       "      <th>DBPM_ADV</th>\n",
       "      <th>VORP_ADV</th>\n",
       "      <th>ORtg_Per100</th>\n",
       "      <th>DRtg_Per100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>-0.388276</td>\n",
       "      <td>-1.249999</td>\n",
       "      <td>-0.983414</td>\n",
       "      <td>-1.289246</td>\n",
       "      <td>-1.395285</td>\n",
       "      <td>-1.577588</td>\n",
       "      <td>-0.967661</td>\n",
       "      <td>-1.300556</td>\n",
       "      <td>-2.070032</td>\n",
       "      <td>-2.482952</td>\n",
       "      <td>-2.527381</td>\n",
       "      <td>-3.562793</td>\n",
       "      <td>0.069473</td>\n",
       "      <td>0.143439</td>\n",
       "      <td>0.078410</td>\n",
       "      <td>0.231760</td>\n",
       "      <td>0.405319</td>\n",
       "      <td>-0.191604</td>\n",
       "      <td>0.884867</td>\n",
       "      <td>-0.648607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>0.358193</td>\n",
       "      <td>-0.153611</td>\n",
       "      <td>-0.939324</td>\n",
       "      <td>-0.320022</td>\n",
       "      <td>-0.365756</td>\n",
       "      <td>-0.438518</td>\n",
       "      <td>0.756131</td>\n",
       "      <td>-0.360851</td>\n",
       "      <td>0.972271</td>\n",
       "      <td>0.508393</td>\n",
       "      <td>0.547324</td>\n",
       "      <td>0.429430</td>\n",
       "      <td>0.712539</td>\n",
       "      <td>-0.603441</td>\n",
       "      <td>-0.872040</td>\n",
       "      <td>-0.253391</td>\n",
       "      <td>0.273167</td>\n",
       "      <td>-0.423988</td>\n",
       "      <td>-0.108764</td>\n",
       "      <td>0.177395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>0.607015</td>\n",
       "      <td>-1.299835</td>\n",
       "      <td>-1.292041</td>\n",
       "      <td>-1.289246</td>\n",
       "      <td>-0.740130</td>\n",
       "      <td>-1.577588</td>\n",
       "      <td>-0.967661</td>\n",
       "      <td>-1.652945</td>\n",
       "      <td>-2.110062</td>\n",
       "      <td>0.194784</td>\n",
       "      <td>-2.206143</td>\n",
       "      <td>-2.973970</td>\n",
       "      <td>-0.111390</td>\n",
       "      <td>-0.155313</td>\n",
       "      <td>-0.792836</td>\n",
       "      <td>0.339572</td>\n",
       "      <td>-0.563797</td>\n",
       "      <td>-0.501449</td>\n",
       "      <td>0.263847</td>\n",
       "      <td>-0.855108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>0.855838</td>\n",
       "      <td>-0.286507</td>\n",
       "      <td>-0.895235</td>\n",
       "      <td>-0.320022</td>\n",
       "      <td>0.570180</td>\n",
       "      <td>-0.628363</td>\n",
       "      <td>-0.393063</td>\n",
       "      <td>-0.948166</td>\n",
       "      <td>-0.348729</td>\n",
       "      <td>0.436022</td>\n",
       "      <td>-0.313134</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>-0.653977</td>\n",
       "      <td>-0.653233</td>\n",
       "      <td>-0.951244</td>\n",
       "      <td>-0.469014</td>\n",
       "      <td>-1.532913</td>\n",
       "      <td>-0.656371</td>\n",
       "      <td>-0.916089</td>\n",
       "      <td>1.003397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>-0.885921</td>\n",
       "      <td>0.660374</td>\n",
       "      <td>1.794234</td>\n",
       "      <td>0.406896</td>\n",
       "      <td>0.663774</td>\n",
       "      <td>1.080241</td>\n",
       "      <td>-0.393063</td>\n",
       "      <td>1.753484</td>\n",
       "      <td>0.932240</td>\n",
       "      <td>0.468187</td>\n",
       "      <td>-0.014842</td>\n",
       "      <td>0.547195</td>\n",
       "      <td>0.190048</td>\n",
       "      <td>-0.454065</td>\n",
       "      <td>-0.476019</td>\n",
       "      <td>-0.334250</td>\n",
       "      <td>1.330385</td>\n",
       "      <td>-0.191604</td>\n",
       "      <td>0.512255</td>\n",
       "      <td>0.383895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year End  PS/G_PerGame  AST_PerGame  ORB_PerGame  DRB_PerGame  \\\n",
       "1840 -0.388276     -1.249999    -0.983414    -1.289246    -1.395285   \n",
       "2907  0.358193     -0.153611    -0.939324    -0.320022    -0.365756   \n",
       "3219  0.607015     -1.299835    -1.292041    -1.289246    -0.740130   \n",
       "3597  0.855838     -0.286507    -0.895235    -0.320022     0.570180   \n",
       "1086 -0.885921      0.660374     1.794234     0.406896     0.663774   \n",
       "\n",
       "      STL_PerGame  BLK_PerGame  TOV_PerGame         G  3P%_Total  2P%_Total  \\\n",
       "1840    -1.577588    -0.967661    -1.300556 -2.070032  -2.482952  -2.527381   \n",
       "2907    -0.438518     0.756131    -0.360851  0.972271   0.508393   0.547324   \n",
       "3219    -1.577588    -0.967661    -1.652945 -2.110062   0.194784  -2.206143   \n",
       "3597    -0.628363    -0.393063    -0.948166 -0.348729   0.436022  -0.313134   \n",
       "1086     1.080241    -0.393063     1.753484  0.932240   0.468187  -0.014842   \n",
       "\n",
       "       TS%_ADV  USG%_ADV   OWS_ADV   DWS_ADV  OBPM_ADV  DBPM_ADV  VORP_ADV  \\\n",
       "1840 -3.562793  0.069473  0.143439  0.078410  0.231760  0.405319 -0.191604   \n",
       "2907  0.429430  0.712539 -0.603441 -0.872040 -0.253391  0.273167 -0.423988   \n",
       "3219 -2.973970 -0.111390 -0.155313 -0.792836  0.339572 -0.563797 -0.501449   \n",
       "3597  0.005477 -0.653977 -0.653233 -0.951244 -0.469014 -1.532913 -0.656371   \n",
       "1086  0.547195  0.190048 -0.454065 -0.476019 -0.334250  1.330385 -0.191604   \n",
       "\n",
       "      ORtg_Per100  DRtg_Per100  \n",
       "1840     0.884867    -0.648607  \n",
       "2907    -0.108764     0.177395  \n",
       "3219     0.263847    -0.855108  \n",
       "3597    -0.916089     1.003397  \n",
       "1086     0.512255     0.383895  "
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = (X_valid - X_mean) / X_std\n",
    "#X_test = (X_test - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optimizer = keras.optimizers.Nadam(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\", input_shape=X_guard_data.shape[1:]),\n",
    "    keras.layers.AlphaDropout(rate=0.15),                             \n",
    "    keras.layers.Dense(50, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.15),                \n",
    "    keras.layers.Dense(4, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=my_optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3233\n",
       "3      40\n",
       "1      40\n",
       "2      39\n",
       "Name: Which Team, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['Which Team'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3352"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03639618138424821"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "122/3352\n",
    "# 3.6% of non-0 team. So naive classifier should get 96.4% right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = len(X) // batch_size * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.axis([10^-5, 10^1, 0, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=my_optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3352 samples\n",
      "Epoch 1/3\n",
      "3352/3352 [==============================] - 1s 265us/sample - loss: 2.0182 - accuracy: 0.2479\n",
      "Epoch 2/3\n",
      "3352/3352 [==============================] - 0s 44us/sample - loss: 1.1829 - accuracy: 0.6113\n",
      "Epoch 3/3\n",
      "3352/3352 [==============================] - 0s 43us/sample - loss: 0.7731 - accuracy: 0.9478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bakhtiyar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Attempted to set non-positive left xlim on a log-scaled axis.\n",
      "Invalid limit will be ignored.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train, y_train, epochs=3, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (iter2 - self.iteration)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights to counter dataset imbalance\n",
    "my_class_weights = {0:1, 1:30, 2:30, 3:30}\n",
    "my_valid_weights = {0:1, 1:30, 2:30, 3:30}\n",
    "valid_sample_weights = y_valid.replace(my_valid_weights).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 800\n",
    "onecycle = OneCycleScheduler(len(X_train) // batch_size * n_epochs, max_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=200,\n",
    "                                                  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3352 samples, validate on 718 samples\n",
      "Epoch 1/800\n",
      "3352/3352 [==============================] - 1s 437us/sample - loss: 3.0952 - accuracy: 0.4305 - val_loss: 2.4877 - val_accuracy: 0.6852\n",
      "Epoch 2/800\n",
      "3352/3352 [==============================] - 1s 164us/sample - loss: 2.4663 - accuracy: 0.6572 - val_loss: 2.4950 - val_accuracy: 0.7841\n",
      "Epoch 3/800\n",
      "3352/3352 [==============================] - 0s 129us/sample - loss: 2.1446 - accuracy: 0.7587 - val_loss: 1.8993 - val_accuracy: 0.8217\n",
      "Epoch 4/800\n",
      "3352/3352 [==============================] - 0s 125us/sample - loss: 1.8607 - accuracy: 0.8180 - val_loss: 1.7289 - val_accuracy: 0.8552\n",
      "Epoch 5/800\n",
      "3352/3352 [==============================] - 1s 153us/sample - loss: 1.8049 - accuracy: 0.8496 - val_loss: 1.6735 - val_accuracy: 0.8733\n",
      "Epoch 6/800\n",
      "3352/3352 [==============================] - 0s 138us/sample - loss: 1.6163 - accuracy: 0.8705 - val_loss: 1.6465 - val_accuracy: 0.8774\n",
      "Epoch 7/800\n",
      "3352/3352 [==============================] - 0s 134us/sample - loss: 1.5908 - accuracy: 0.8696 - val_loss: 1.8865 - val_accuracy: 0.8844\n",
      "Epoch 8/800\n",
      "3352/3352 [==============================] - 0s 140us/sample - loss: 1.4041 - accuracy: 0.8923 - val_loss: 1.8069 - val_accuracy: 0.8760\n",
      "Epoch 9/800\n",
      "3352/3352 [==============================] - 0s 120us/sample - loss: 1.5731 - accuracy: 0.8863 - val_loss: 1.7422 - val_accuracy: 0.8886\n",
      "Epoch 10/800\n",
      "3352/3352 [==============================] - 0s 144us/sample - loss: 1.4730 - accuracy: 0.8971 - val_loss: 1.6299 - val_accuracy: 0.9011\n",
      "Epoch 11/800\n",
      "3352/3352 [==============================] - 0s 144us/sample - loss: 1.4083 - accuracy: 0.9013 - val_loss: 1.8845 - val_accuracy: 0.8969\n",
      "Epoch 12/800\n",
      "3352/3352 [==============================] - 0s 134us/sample - loss: 1.4142 - accuracy: 0.9018 - val_loss: 1.6567 - val_accuracy: 0.9067\n",
      "Epoch 13/800\n",
      "3352/3352 [==============================] - 0s 136us/sample - loss: 1.2908 - accuracy: 0.9192 - val_loss: 1.4782 - val_accuracy: 0.9109\n",
      "Epoch 14/800\n",
      "3352/3352 [==============================] - 0s 136us/sample - loss: 1.4252 - accuracy: 0.8986 - val_loss: 1.4595 - val_accuracy: 0.9164\n",
      "Epoch 15/800\n",
      "3352/3352 [==============================] - 0s 140us/sample - loss: 1.2519 - accuracy: 0.9159 - val_loss: 1.5826 - val_accuracy: 0.9011\n",
      "Epoch 16/800\n",
      "3352/3352 [==============================] - 0s 137us/sample - loss: 1.3133 - accuracy: 0.9087 - val_loss: 1.6024 - val_accuracy: 0.8900\n",
      "Epoch 17/800\n",
      "3352/3352 [==============================] - 0s 135us/sample - loss: 1.2763 - accuracy: 0.9024 - val_loss: 1.6799 - val_accuracy: 0.9011\n",
      "Epoch 18/800\n",
      "3352/3352 [==============================] - 0s 120us/sample - loss: 1.1858 - accuracy: 0.9162 - val_loss: 1.7330 - val_accuracy: 0.8872\n",
      "Epoch 19/800\n",
      "3352/3352 [==============================] - 1s 162us/sample - loss: 1.2555 - accuracy: 0.9102 - val_loss: 1.6987 - val_accuracy: 0.9011\n",
      "Epoch 20/800\n",
      "3352/3352 [==============================] - 0s 137us/sample - loss: 1.2166 - accuracy: 0.9147 - val_loss: 1.6622 - val_accuracy: 0.8983\n",
      "Epoch 21/800\n",
      "3352/3352 [==============================] - 1s 165us/sample - loss: 1.2402 - accuracy: 0.9165 - val_loss: 1.6595 - val_accuracy: 0.8997\n",
      "Epoch 22/800\n",
      "3352/3352 [==============================] - 0s 139us/sample - loss: 1.1473 - accuracy: 0.9206 - val_loss: 1.6645 - val_accuracy: 0.8969\n",
      "Epoch 23/800\n",
      "3352/3352 [==============================] - 0s 137us/sample - loss: 1.1492 - accuracy: 0.9138 - val_loss: 1.7892 - val_accuracy: 0.8997\n",
      "Epoch 24/800\n",
      "3352/3352 [==============================] - 1s 151us/sample - loss: 1.1103 - accuracy: 0.9227 - val_loss: 1.8974 - val_accuracy: 0.8830\n",
      "Epoch 25/800\n",
      "3352/3352 [==============================] - 0s 132us/sample - loss: 1.1105 - accuracy: 0.9218 - val_loss: 1.7680 - val_accuracy: 0.8997\n",
      "Epoch 26/800\n",
      "3352/3352 [==============================] - 0s 122us/sample - loss: 1.0649 - accuracy: 0.9206 - val_loss: 1.7182 - val_accuracy: 0.9067\n",
      "Epoch 27/800\n",
      "3352/3352 [==============================] - 1s 162us/sample - loss: 1.0658 - accuracy: 0.9203 - val_loss: 1.8228 - val_accuracy: 0.9150\n",
      "Epoch 28/800\n",
      "3352/3352 [==============================] - 0s 123us/sample - loss: 1.0565 - accuracy: 0.9236 - val_loss: 1.7693 - val_accuracy: 0.9081\n",
      "Epoch 29/800\n",
      "3352/3352 [==============================] - 0s 134us/sample - loss: 0.9919 - accuracy: 0.9278 - val_loss: 1.7012 - val_accuracy: 0.9136\n",
      "Epoch 30/800\n",
      "3352/3352 [==============================] - 0s 147us/sample - loss: 1.0987 - accuracy: 0.9239 - val_loss: 1.7189 - val_accuracy: 0.8983\n",
      "Epoch 31/800\n",
      "3352/3352 [==============================] - 0s 128us/sample - loss: 0.9961 - accuracy: 0.9227 - val_loss: 1.6772 - val_accuracy: 0.8928\n",
      "Epoch 32/800\n",
      "3352/3352 [==============================] - 0s 144us/sample - loss: 0.9786 - accuracy: 0.9302 - val_loss: 2.0250 - val_accuracy: 0.8928\n",
      "Epoch 33/800\n",
      "3352/3352 [==============================] - 0s 132us/sample - loss: 0.9540 - accuracy: 0.9209 - val_loss: 1.7393 - val_accuracy: 0.9067\n",
      "Epoch 34/800\n",
      "3352/3352 [==============================] - 0s 136us/sample - loss: 0.9750 - accuracy: 0.9293 - val_loss: 1.9576 - val_accuracy: 0.9039\n",
      "Epoch 35/800\n",
      "3352/3352 [==============================] - 0s 130us/sample - loss: 1.0083 - accuracy: 0.9230 - val_loss: 1.7812 - val_accuracy: 0.8955\n",
      "Epoch 36/800\n",
      "3352/3352 [==============================] - 1s 157us/sample - loss: 0.9532 - accuracy: 0.9197 - val_loss: 1.8168 - val_accuracy: 0.8942\n",
      "Epoch 37/800\n",
      "3352/3352 [==============================] - 0s 117us/sample - loss: 0.8587 - accuracy: 0.9257 - val_loss: 1.9711 - val_accuracy: 0.8997\n",
      "Epoch 38/800\n",
      "3352/3352 [==============================] - 1s 151us/sample - loss: 0.9081 - accuracy: 0.9248 - val_loss: 2.0075 - val_accuracy: 0.8983\n",
      "Epoch 39/800\n",
      "3352/3352 [==============================] - 0s 131us/sample - loss: 0.9260 - accuracy: 0.9317 - val_loss: 1.9521 - val_accuracy: 0.9039\n",
      "Epoch 40/800\n",
      "3352/3352 [==============================] - 0s 125us/sample - loss: 0.9479 - accuracy: 0.9320 - val_loss: 2.0375 - val_accuracy: 0.8942\n",
      "Epoch 41/800\n",
      "3352/3352 [==============================] - 1s 152us/sample - loss: 0.8450 - accuracy: 0.9260 - val_loss: 2.0751 - val_accuracy: 0.9011\n",
      "Epoch 42/800\n",
      "3352/3352 [==============================] - 0s 129us/sample - loss: 0.9672 - accuracy: 0.9251 - val_loss: 2.0513 - val_accuracy: 0.9039\n",
      "Epoch 43/800\n",
      "3352/3352 [==============================] - 0s 146us/sample - loss: 0.8668 - accuracy: 0.9332 - val_loss: 2.1315 - val_accuracy: 0.9136\n",
      "Epoch 44/800\n",
      "3352/3352 [==============================] - 1s 196us/sample - loss: 0.9163 - accuracy: 0.9308 - val_loss: 1.9748 - val_accuracy: 0.9025\n",
      "Epoch 45/800\n",
      "3352/3352 [==============================] - 0s 131us/sample - loss: 0.9225 - accuracy: 0.9263 - val_loss: 2.0133 - val_accuracy: 0.8955\n",
      "Epoch 46/800\n",
      "3352/3352 [==============================] - 1s 159us/sample - loss: 0.8136 - accuracy: 0.9320 - val_loss: 2.0441 - val_accuracy: 0.9025\n",
      "Epoch 47/800\n",
      "3352/3352 [==============================] - 0s 131us/sample - loss: 0.9003 - accuracy: 0.9302 - val_loss: 1.9562 - val_accuracy: 0.9011\n",
      "Epoch 48/800\n",
      "3352/3352 [==============================] - 0s 138us/sample - loss: 0.8287 - accuracy: 0.9302 - val_loss: 1.8156 - val_accuracy: 0.9025\n",
      "Epoch 49/800\n",
      "3352/3352 [==============================] - 0s 149us/sample - loss: 0.8518 - accuracy: 0.9368 - val_loss: 1.9560 - val_accuracy: 0.9123\n",
      "Epoch 50/800\n",
      "3352/3352 [==============================] - 0s 134us/sample - loss: 0.8590 - accuracy: 0.9323 - val_loss: 2.1632 - val_accuracy: 0.9123\n",
      "Epoch 51/800\n",
      "3352/3352 [==============================] - 0s 131us/sample - loss: 0.8419 - accuracy: 0.9341 - val_loss: 2.1969 - val_accuracy: 0.9164\n",
      "Epoch 52/800\n",
      "3352/3352 [==============================] - 0s 142us/sample - loss: 0.8357 - accuracy: 0.9269 - val_loss: 2.2374 - val_accuracy: 0.9081\n",
      "Epoch 53/800\n",
      "3352/3352 [==============================] - 0s 138us/sample - loss: 0.8233 - accuracy: 0.9347 - val_loss: 2.1153 - val_accuracy: 0.9206\n",
      "Epoch 54/800\n",
      "3352/3352 [==============================] - 0s 117us/sample - loss: 0.7784 - accuracy: 0.9365 - val_loss: 2.2351 - val_accuracy: 0.9081\n",
      "Epoch 55/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3352/3352 [==============================] - 0s 146us/sample - loss: 0.7730 - accuracy: 0.9359 - val_loss: 2.0929 - val_accuracy: 0.9081\n",
      "Epoch 56/800\n",
      "3352/3352 [==============================] - 0s 121us/sample - loss: 0.7609 - accuracy: 0.9311 - val_loss: 2.2486 - val_accuracy: 0.9095\n",
      "Epoch 57/800\n",
      "3352/3352 [==============================] - 0s 135us/sample - loss: 0.7514 - accuracy: 0.9326 - val_loss: 2.1289 - val_accuracy: 0.9136\n",
      "Epoch 58/800\n",
      "3352/3352 [==============================] - 0s 142us/sample - loss: 0.7878 - accuracy: 0.9359 - val_loss: 2.2285 - val_accuracy: 0.9011\n",
      "Epoch 59/800\n",
      "3352/3352 [==============================] - 0s 122us/sample - loss: 0.7723 - accuracy: 0.9329 - val_loss: 2.3112 - val_accuracy: 0.9039\n",
      "Epoch 60/800\n",
      "3352/3352 [==============================] - 0s 124us/sample - loss: 0.7038 - accuracy: 0.9374 - val_loss: 2.3411 - val_accuracy: 0.9067\n",
      "Epoch 61/800\n",
      "3352/3352 [==============================] - 0s 140us/sample - loss: 0.7796 - accuracy: 0.9317 - val_loss: 2.2290 - val_accuracy: 0.9011\n",
      "Epoch 62/800\n",
      "3352/3352 [==============================] - 0s 123us/sample - loss: 0.7374 - accuracy: 0.9299 - val_loss: 2.2436 - val_accuracy: 0.9039\n",
      "Epoch 63/800\n",
      "3352/3352 [==============================] - 0s 124us/sample - loss: 0.6619 - accuracy: 0.9353 - val_loss: 2.2552 - val_accuracy: 0.9081\n",
      "Epoch 64/800\n",
      "3352/3352 [==============================] - 1s 152us/sample - loss: 0.6823 - accuracy: 0.9385 - val_loss: 2.3952 - val_accuracy: 0.9192\n",
      "Epoch 65/800\n",
      "3352/3352 [==============================] - 0s 120us/sample - loss: 0.6221 - accuracy: 0.9385 - val_loss: 2.3017 - val_accuracy: 0.9123\n",
      "Epoch 66/800\n",
      "3352/3352 [==============================] - 0s 143us/sample - loss: 0.7438 - accuracy: 0.9338 - val_loss: 2.3218 - val_accuracy: 0.9095\n",
      "Epoch 67/800\n",
      "3352/3352 [==============================] - 0s 143us/sample - loss: 0.6158 - accuracy: 0.9385 - val_loss: 2.5948 - val_accuracy: 0.9011\n",
      "Epoch 68/800\n",
      "3352/3352 [==============================] - 1s 153us/sample - loss: 0.6482 - accuracy: 0.9460 - val_loss: 2.5530 - val_accuracy: 0.9136\n",
      "Epoch 69/800\n",
      "3352/3352 [==============================] - 0s 147us/sample - loss: 0.7137 - accuracy: 0.9341 - val_loss: 2.4116 - val_accuracy: 0.9123\n",
      "Epoch 70/800\n",
      "3352/3352 [==============================] - 0s 123us/sample - loss: 0.6251 - accuracy: 0.9400 - val_loss: 2.6856 - val_accuracy: 0.9164\n",
      "Epoch 71/800\n",
      "3352/3352 [==============================] - 0s 126us/sample - loss: 0.6126 - accuracy: 0.9424 - val_loss: 2.6916 - val_accuracy: 0.9220\n",
      "Epoch 72/800\n",
      "3352/3352 [==============================] - 0s 136us/sample - loss: 0.6708 - accuracy: 0.9388 - val_loss: 2.6781 - val_accuracy: 0.9234\n",
      "Epoch 73/800\n",
      "3352/3352 [==============================] - 0s 116us/sample - loss: 0.5415 - accuracy: 0.9487 - val_loss: 2.7733 - val_accuracy: 0.9248\n",
      "Epoch 74/800\n",
      "3352/3352 [==============================] - 0s 124us/sample - loss: 0.6169 - accuracy: 0.9469 - val_loss: 2.5858 - val_accuracy: 0.9081\n",
      "Epoch 75/800\n",
      "3352/3352 [==============================] - 1s 151us/sample - loss: 0.5912 - accuracy: 0.9374 - val_loss: 2.6920 - val_accuracy: 0.9178\n",
      "Epoch 76/800\n",
      "3352/3352 [==============================] - 0s 121us/sample - loss: 0.5956 - accuracy: 0.9421 - val_loss: 2.5793 - val_accuracy: 0.9248\n",
      "Epoch 77/800\n",
      "3352/3352 [==============================] - 0s 122us/sample - loss: 0.5610 - accuracy: 0.9466 - val_loss: 2.7364 - val_accuracy: 0.9178\n",
      "Epoch 78/800\n",
      "3352/3352 [==============================] - 0s 134us/sample - loss: 0.6662 - accuracy: 0.9362 - val_loss: 2.8385 - val_accuracy: 0.9067\n",
      "Epoch 79/800\n",
      "3352/3352 [==============================] - 0s 139us/sample - loss: 0.6550 - accuracy: 0.9400 - val_loss: 2.8845 - val_accuracy: 0.9136\n",
      "Epoch 80/800\n",
      "3352/3352 [==============================] - 0s 124us/sample - loss: 0.5524 - accuracy: 0.9451 - val_loss: 2.8680 - val_accuracy: 0.9123\n",
      "Epoch 81/800\n",
      "3352/3352 [==============================] - 0s 145us/sample - loss: 0.4703 - accuracy: 0.9478 - val_loss: 2.9885 - val_accuracy: 0.9220\n",
      "Epoch 82/800\n",
      "3352/3352 [==============================] - 0s 119us/sample - loss: 0.5668 - accuracy: 0.9469 - val_loss: 2.9866 - val_accuracy: 0.9150\n",
      "Epoch 83/800\n",
      "3352/3352 [==============================] - 0s 123us/sample - loss: 0.5203 - accuracy: 0.9496 - val_loss: 2.8824 - val_accuracy: 0.9220\n",
      "Epoch 84/800\n",
      "3352/3352 [==============================] - 1s 157us/sample - loss: 0.5361 - accuracy: 0.9454 - val_loss: 2.9227 - val_accuracy: 0.9178\n",
      "Epoch 85/800\n",
      "3352/3352 [==============================] - 0s 114us/sample - loss: 0.6341 - accuracy: 0.9430 - val_loss: 3.1061 - val_accuracy: 0.9081\n",
      "Epoch 86/800\n",
      "3352/3352 [==============================] - 0s 142us/sample - loss: 0.5460 - accuracy: 0.9466 - val_loss: 3.1509 - val_accuracy: 0.9220\n",
      "Epoch 87/800\n",
      "3352/3352 [==============================] - 0s 140us/sample - loss: 0.5326 - accuracy: 0.9469 - val_loss: 3.1341 - val_accuracy: 0.9220\n",
      "Epoch 88/800\n",
      "3352/3352 [==============================] - 0s 123us/sample - loss: 0.5851 - accuracy: 0.9484 - val_loss: 2.8770 - val_accuracy: 0.9234\n",
      "Epoch 89/800\n",
      "3352/3352 [==============================] - 0s 145us/sample - loss: 0.5744 - accuracy: 0.9448 - val_loss: 3.0226 - val_accuracy: 0.9178\n",
      "Epoch 90/800\n",
      "3352/3352 [==============================] - 0s 125us/sample - loss: 0.5655 - accuracy: 0.9430 - val_loss: 3.1453 - val_accuracy: 0.9039\n",
      "Epoch 91/800\n",
      "3352/3352 [==============================] - 0s 127us/sample - loss: 0.4540 - accuracy: 0.9475 - val_loss: 3.1030 - val_accuracy: 0.9136\n",
      "Epoch 92/800\n",
      "3352/3352 [==============================] - 1s 167us/sample - loss: 0.4760 - accuracy: 0.9412 - val_loss: 3.2901 - val_accuracy: 0.9206\n",
      "Epoch 93/800\n",
      "3352/3352 [==============================] - 0s 139us/sample - loss: 0.5039 - accuracy: 0.9451 - val_loss: 3.3132 - val_accuracy: 0.9206\n",
      "Epoch 94/800\n",
      "3352/3352 [==============================] - 0s 132us/sample - loss: 0.4751 - accuracy: 0.9523 - val_loss: 3.1093 - val_accuracy: 0.9150\n",
      "Epoch 95/800\n",
      "3352/3352 [==============================] - 1s 159us/sample - loss: 0.4788 - accuracy: 0.9469 - val_loss: 3.2691 - val_accuracy: 0.9318\n",
      "Epoch 96/800\n",
      "3352/3352 [==============================] - 0s 127us/sample - loss: 0.4503 - accuracy: 0.9484 - val_loss: 3.2081 - val_accuracy: 0.9192\n",
      "Epoch 97/800\n",
      "3352/3352 [==============================] - 0s 135us/sample - loss: 0.5130 - accuracy: 0.9463 - val_loss: 3.1062 - val_accuracy: 0.9248\n",
      "Epoch 98/800\n",
      "3352/3352 [==============================] - 0s 141us/sample - loss: 0.4821 - accuracy: 0.9442 - val_loss: 3.3012 - val_accuracy: 0.9192\n",
      "Epoch 99/800\n",
      "3352/3352 [==============================] - 0s 135us/sample - loss: 0.5311 - accuracy: 0.9445 - val_loss: 3.2996 - val_accuracy: 0.9220\n",
      "Epoch 100/800\n",
      "3352/3352 [==============================] - 0s 124us/sample - loss: 0.5299 - accuracy: 0.9487 - val_loss: 3.2297 - val_accuracy: 0.9150\n",
      "Epoch 101/800\n",
      "3352/3352 [==============================] - 0s 148us/sample - loss: 0.5185 - accuracy: 0.9463 - val_loss: 3.0669 - val_accuracy: 0.9123\n",
      "Epoch 102/800\n",
      "3352/3352 [==============================] - 0s 121us/sample - loss: 0.4428 - accuracy: 0.9493 - val_loss: 3.4150 - val_accuracy: 0.9234\n",
      "Epoch 103/800\n",
      "3352/3352 [==============================] - 0s 139us/sample - loss: 0.4556 - accuracy: 0.9490 - val_loss: 3.3785 - val_accuracy: 0.9192\n",
      "Epoch 104/800\n",
      "3352/3352 [==============================] - 0s 134us/sample - loss: 0.4497 - accuracy: 0.9493 - val_loss: 3.4262 - val_accuracy: 0.9206\n",
      "Epoch 105/800\n",
      "3352/3352 [==============================] - 0s 119us/sample - loss: 0.4681 - accuracy: 0.9514 - val_loss: 3.4782 - val_accuracy: 0.9234\n",
      "Epoch 106/800\n",
      "3352/3352 [==============================] - 1s 169us/sample - loss: 0.4317 - accuracy: 0.9481 - val_loss: 3.3444 - val_accuracy: 0.9262\n",
      "Epoch 107/800\n",
      "3352/3352 [==============================] - 0s 117us/sample - loss: 0.3402 - accuracy: 0.9541 - val_loss: 3.5021 - val_accuracy: 0.9234\n",
      "Epoch 108/800\n",
      "3352/3352 [==============================] - 0s 125us/sample - loss: 0.5217 - accuracy: 0.9487 - val_loss: 3.4877 - val_accuracy: 0.9276\n",
      "Epoch 109/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3352/3352 [==============================] - 0s 147us/sample - loss: 0.3764 - accuracy: 0.9538 - val_loss: 3.4484 - val_accuracy: 0.9192\n",
      "Epoch 110/800\n",
      "3352/3352 [==============================] - 0s 123us/sample - loss: 0.4019 - accuracy: 0.9523 - val_loss: 3.5674 - val_accuracy: 0.9109\n",
      "Epoch 111/800\n",
      "3352/3352 [==============================] - 0s 115us/sample - loss: 0.4095 - accuracy: 0.9520 - val_loss: 3.3873 - val_accuracy: 0.9220\n",
      "Epoch 112/800\n",
      "3352/3352 [==============================] - 1s 162us/sample - loss: 0.3884 - accuracy: 0.9493 - val_loss: 3.4805 - val_accuracy: 0.9220\n",
      "Epoch 113/800\n",
      "3352/3352 [==============================] - 0s 113us/sample - loss: 0.4332 - accuracy: 0.9550 - val_loss: 3.4388 - val_accuracy: 0.9164\n",
      "Epoch 114/800\n",
      "3352/3352 [==============================] - 0s 126us/sample - loss: 0.4679 - accuracy: 0.9520 - val_loss: 3.4235 - val_accuracy: 0.9262\n",
      "Epoch 115/800\n",
      "3352/3352 [==============================] - 1s 156us/sample - loss: 0.4064 - accuracy: 0.9520 - val_loss: 3.4049 - val_accuracy: 0.9206\n",
      "Epoch 116/800\n",
      "3352/3352 [==============================] - 0s 138us/sample - loss: 0.3987 - accuracy: 0.9505 - val_loss: 3.4516 - val_accuracy: 0.9178\n",
      "Epoch 117/800\n",
      "3352/3352 [==============================] - 1s 160us/sample - loss: 0.4548 - accuracy: 0.9475 - val_loss: 3.4094 - val_accuracy: 0.9123\n",
      "Epoch 118/800\n",
      "3352/3352 [==============================] - 1s 171us/sample - loss: 0.3873 - accuracy: 0.9517 - val_loss: 3.6102 - val_accuracy: 0.9192\n",
      "Epoch 119/800\n",
      "3352/3352 [==============================] - 1s 152us/sample - loss: 0.4572 - accuracy: 0.9484 - val_loss: 3.6790 - val_accuracy: 0.9178\n",
      "Epoch 120/800\n",
      "3352/3352 [==============================] - 1s 162us/sample - loss: 0.4846 - accuracy: 0.9490 - val_loss: 3.5145 - val_accuracy: 0.9150\n",
      "Epoch 121/800\n",
      "3352/3352 [==============================] - 0s 146us/sample - loss: 0.4513 - accuracy: 0.9490 - val_loss: 3.2797 - val_accuracy: 0.9206\n",
      "Epoch 122/800\n",
      "3352/3352 [==============================] - 1s 162us/sample - loss: 0.4351 - accuracy: 0.9499 - val_loss: 3.3999 - val_accuracy: 0.9192\n",
      "Epoch 123/800\n",
      "3352/3352 [==============================] - 0s 137us/sample - loss: 0.4289 - accuracy: 0.9448 - val_loss: 3.4807 - val_accuracy: 0.9234\n",
      "Epoch 124/800\n",
      "3352/3352 [==============================] - 0s 120us/sample - loss: 0.4631 - accuracy: 0.9499 - val_loss: 3.5294 - val_accuracy: 0.9234\n",
      "Epoch 125/800\n",
      "3352/3352 [==============================] - 1s 157us/sample - loss: 0.4156 - accuracy: 0.9517 - val_loss: 3.6381 - val_accuracy: 0.9150\n",
      "Epoch 126/800\n",
      "3352/3352 [==============================] - 1s 156us/sample - loss: 0.3587 - accuracy: 0.9517 - val_loss: 3.6538 - val_accuracy: 0.9262\n",
      "Epoch 127/800\n",
      "3352/3352 [==============================] - 0s 119us/sample - loss: 0.3339 - accuracy: 0.9561 - val_loss: 3.8086 - val_accuracy: 0.9248\n",
      "Epoch 128/800\n",
      "3352/3352 [==============================] - 0s 144us/sample - loss: 0.3529 - accuracy: 0.9523 - val_loss: 3.6091 - val_accuracy: 0.9234\n",
      "Epoch 129/800\n",
      "3352/3352 [==============================] - 0s 124us/sample - loss: 0.3802 - accuracy: 0.9541 - val_loss: 3.6328 - val_accuracy: 0.9262\n",
      "Epoch 130/800\n",
      "3352/3352 [==============================] - 0s 118us/sample - loss: 0.3678 - accuracy: 0.9544 - val_loss: 3.6481 - val_accuracy: 0.9164\n",
      "Epoch 131/800\n",
      "3352/3352 [==============================] - 1s 164us/sample - loss: 0.3700 - accuracy: 0.9484 - val_loss: 3.6244 - val_accuracy: 0.9178\n",
      "Epoch 132/800\n",
      "3352/3352 [==============================] - 0s 113us/sample - loss: 0.4091 - accuracy: 0.9588 - val_loss: 3.7215 - val_accuracy: 0.9220\n",
      "Epoch 133/800\n",
      "3352/3352 [==============================] - 0s 140us/sample - loss: 0.3840 - accuracy: 0.9517 - val_loss: 3.7254 - val_accuracy: 0.9178\n",
      "Epoch 134/800\n",
      "3352/3352 [==============================] - 0s 138us/sample - loss: 0.3802 - accuracy: 0.9529 - val_loss: 3.8978 - val_accuracy: 0.9206\n",
      "Epoch 135/800\n",
      "3352/3352 [==============================] - 0s 132us/sample - loss: 0.3772 - accuracy: 0.9573 - val_loss: 3.5452 - val_accuracy: 0.9234\n",
      "Epoch 136/800\n",
      "3352/3352 [==============================] - 0s 135us/sample - loss: 0.3976 - accuracy: 0.9505 - val_loss: 3.6276 - val_accuracy: 0.9206\n",
      "Epoch 137/800\n",
      "3352/3352 [==============================] - 0s 144us/sample - loss: 0.3179 - accuracy: 0.9520 - val_loss: 3.8542 - val_accuracy: 0.9234\n",
      "Epoch 138/800\n",
      "3352/3352 [==============================] - 1s 151us/sample - loss: 0.4312 - accuracy: 0.9550 - val_loss: 3.7342 - val_accuracy: 0.9164\n",
      "Epoch 139/800\n",
      "3352/3352 [==============================] - 1s 159us/sample - loss: 0.3079 - accuracy: 0.9553 - val_loss: 3.7222 - val_accuracy: 0.9192\n",
      "Epoch 140/800\n",
      "3352/3352 [==============================] - 0s 127us/sample - loss: 0.3250 - accuracy: 0.9594 - val_loss: 3.6004 - val_accuracy: 0.9234\n",
      "Epoch 141/800\n",
      "3352/3352 [==============================] - 0s 121us/sample - loss: 0.3931 - accuracy: 0.9511 - val_loss: 3.7264 - val_accuracy: 0.9220\n",
      "Epoch 142/800\n",
      "3352/3352 [==============================] - 0s 138us/sample - loss: 0.4016 - accuracy: 0.9579 - val_loss: 3.8155 - val_accuracy: 0.9220\n",
      "Epoch 143/800\n",
      "3352/3352 [==============================] - 0s 126us/sample - loss: 0.3502 - accuracy: 0.9612 - val_loss: 3.6569 - val_accuracy: 0.9262\n",
      "Epoch 144/800\n",
      "3352/3352 [==============================] - 0s 137us/sample - loss: 0.3088 - accuracy: 0.9576 - val_loss: 3.8875 - val_accuracy: 0.9220\n",
      "Epoch 145/800\n",
      "3352/3352 [==============================] - 0s 140us/sample - loss: 0.3603 - accuracy: 0.9561 - val_loss: 3.7743 - val_accuracy: 0.9206\n",
      "Epoch 146/800\n",
      "3352/3352 [==============================] - 0s 130us/sample - loss: 0.3606 - accuracy: 0.9484 - val_loss: 3.5783 - val_accuracy: 0.9206\n",
      "Epoch 147/800\n",
      "3352/3352 [==============================] - 0s 118us/sample - loss: 0.3407 - accuracy: 0.9493 - val_loss: 3.7914 - val_accuracy: 0.9206\n",
      "Epoch 148/800\n",
      "3352/3352 [==============================] - 1s 159us/sample - loss: 0.4407 - accuracy: 0.9532 - val_loss: 3.7774 - val_accuracy: 0.9192\n",
      "Epoch 149/800\n",
      "3352/3352 [==============================] - 0s 118us/sample - loss: 0.2805 - accuracy: 0.9591 - val_loss: 3.6951 - val_accuracy: 0.9248\n",
      "Epoch 150/800\n",
      "3352/3352 [==============================] - 0s 123us/sample - loss: 0.3039 - accuracy: 0.9553 - val_loss: 3.9412 - val_accuracy: 0.9206\n",
      "Epoch 151/800\n",
      "3352/3352 [==============================] - 1s 150us/sample - loss: 0.2970 - accuracy: 0.9585 - val_loss: 3.9072 - val_accuracy: 0.9262\n",
      "Epoch 152/800\n",
      "3352/3352 [==============================] - 0s 120us/sample - loss: 0.3027 - accuracy: 0.9594 - val_loss: 3.9949 - val_accuracy: 0.9234\n",
      "Epoch 153/800\n",
      "3352/3352 [==============================] - 0s 139us/sample - loss: 0.3629 - accuracy: 0.9553 - val_loss: 3.8429 - val_accuracy: 0.9220\n",
      "Epoch 154/800\n",
      "3352/3352 [==============================] - 0s 133us/sample - loss: 0.3483 - accuracy: 0.9618 - val_loss: 3.8066 - val_accuracy: 0.9234\n",
      "Epoch 155/800\n",
      "3352/3352 [==============================] - 0s 131us/sample - loss: 0.3885 - accuracy: 0.9517 - val_loss: 4.0680 - val_accuracy: 0.9248\n",
      "Epoch 156/800\n",
      "3352/3352 [==============================] - 0s 139us/sample - loss: 0.3146 - accuracy: 0.9591 - val_loss: 3.9102 - val_accuracy: 0.9262\n",
      "Epoch 157/800\n",
      "3352/3352 [==============================] - 0s 143us/sample - loss: 0.3921 - accuracy: 0.9538 - val_loss: 3.8016 - val_accuracy: 0.9248\n",
      "Epoch 158/800\n",
      "3352/3352 [==============================] - 0s 126us/sample - loss: 0.3158 - accuracy: 0.9585 - val_loss: 3.9323 - val_accuracy: 0.9276\n",
      "Epoch 159/800\n",
      "3352/3352 [==============================] - 0s 141us/sample - loss: 0.3960 - accuracy: 0.9591 - val_loss: 3.8809 - val_accuracy: 0.9318\n",
      "Epoch 160/800\n",
      "3352/3352 [==============================] - 0s 121us/sample - loss: 0.3104 - accuracy: 0.9621 - val_loss: 4.0791 - val_accuracy: 0.9234\n",
      "Epoch 161/800\n",
      "3352/3352 [==============================] - 0s 144us/sample - loss: 0.3197 - accuracy: 0.9547 - val_loss: 4.0518 - val_accuracy: 0.9220\n",
      "Epoch 162/800\n",
      "3352/3352 [==============================] - 1s 170us/sample - loss: 0.3457 - accuracy: 0.9591 - val_loss: 4.0251 - val_accuracy: 0.9192\n",
      "Epoch 163/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3352/3352 [==============================] - 0s 142us/sample - loss: 0.3058 - accuracy: 0.9594 - val_loss: 4.1329 - val_accuracy: 0.9206\n",
      "Epoch 164/800\n",
      "3352/3352 [==============================] - 0s 115us/sample - loss: 0.2991 - accuracy: 0.9588 - val_loss: 3.9511 - val_accuracy: 0.9248\n",
      "Epoch 165/800\n",
      "3352/3352 [==============================] - 0s 143us/sample - loss: 0.3478 - accuracy: 0.9630 - val_loss: 4.1689 - val_accuracy: 0.9234\n",
      "Epoch 166/800\n",
      "3352/3352 [==============================] - 0s 140us/sample - loss: 0.3460 - accuracy: 0.9627 - val_loss: 3.9669 - val_accuracy: 0.9192\n",
      "Epoch 167/800\n",
      "3352/3352 [==============================] - 0s 121us/sample - loss: 0.3816 - accuracy: 0.9573 - val_loss: 3.8921 - val_accuracy: 0.9234\n",
      "Epoch 168/800\n",
      "3352/3352 [==============================] - 0s 149us/sample - loss: 0.3081 - accuracy: 0.9603 - val_loss: 4.0690 - val_accuracy: 0.9234\n",
      "Epoch 169/800\n",
      "3352/3352 [==============================] - 0s 117us/sample - loss: 0.2942 - accuracy: 0.9612 - val_loss: 4.0715 - val_accuracy: 0.9220\n",
      "Epoch 170/800\n",
      "3352/3352 [==============================] - 0s 137us/sample - loss: 0.2957 - accuracy: 0.9585 - val_loss: 4.0455 - val_accuracy: 0.9248\n",
      "Epoch 171/800\n",
      "3352/3352 [==============================] - 0s 138us/sample - loss: 0.3295 - accuracy: 0.9615 - val_loss: 4.0399 - val_accuracy: 0.9248\n",
      "Epoch 172/800\n",
      "3352/3352 [==============================] - 0s 122us/sample - loss: 0.3009 - accuracy: 0.9567 - val_loss: 4.0796 - val_accuracy: 0.9262\n",
      "Epoch 173/800\n",
      "3352/3352 [==============================] - 0s 148us/sample - loss: 0.3229 - accuracy: 0.9558 - val_loss: 4.1064 - val_accuracy: 0.9276\n",
      "Epoch 174/800\n",
      "3352/3352 [==============================] - 0s 133us/sample - loss: 0.2989 - accuracy: 0.9606 - val_loss: 3.9359 - val_accuracy: 0.9276\n",
      "Epoch 175/800\n",
      "3352/3352 [==============================] - 0s 129us/sample - loss: 0.2638 - accuracy: 0.9576 - val_loss: 4.0638 - val_accuracy: 0.9248\n",
      "Epoch 176/800\n",
      "3352/3352 [==============================] - 0s 138us/sample - loss: 0.3060 - accuracy: 0.9627 - val_loss: 4.0582 - val_accuracy: 0.9276\n",
      "Epoch 177/800\n",
      "3352/3352 [==============================] - 0s 143us/sample - loss: 0.2867 - accuracy: 0.9597 - val_loss: 3.9409 - val_accuracy: 0.9234\n",
      "Epoch 178/800\n",
      "3352/3352 [==============================] - 0s 121us/sample - loss: 0.2409 - accuracy: 0.9669 - val_loss: 4.2255 - val_accuracy: 0.9248\n",
      "Epoch 179/800\n",
      "3352/3352 [==============================] - 1s 177us/sample - loss: 0.3206 - accuracy: 0.9639 - val_loss: 4.1585 - val_accuracy: 0.9262\n",
      "Epoch 180/800\n",
      "3352/3352 [==============================] - 0s 119us/sample - loss: 0.2189 - accuracy: 0.9618 - val_loss: 4.1168 - val_accuracy: 0.9304\n",
      "Epoch 181/800\n",
      "3352/3352 [==============================] - 0s 124us/sample - loss: 0.2888 - accuracy: 0.9690 - val_loss: 4.2646 - val_accuracy: 0.9220\n",
      "Epoch 182/800\n",
      "3352/3352 [==============================] - 0s 134us/sample - loss: 0.2862 - accuracy: 0.9627 - val_loss: 4.0350 - val_accuracy: 0.9276\n",
      "Epoch 183/800\n",
      "3352/3352 [==============================] - 0s 135us/sample - loss: 0.3272 - accuracy: 0.9603 - val_loss: 4.1370 - val_accuracy: 0.9248\n",
      "Epoch 184/800\n",
      "3352/3352 [==============================] - 0s 120us/sample - loss: 0.2341 - accuracy: 0.9621 - val_loss: 4.1890 - val_accuracy: 0.9234\n",
      "Epoch 185/800\n",
      "3352/3352 [==============================] - 1s 156us/sample - loss: 0.2638 - accuracy: 0.9636 - val_loss: 4.2602 - val_accuracy: 0.9276\n",
      "Epoch 186/800\n",
      "3352/3352 [==============================] - 1s 150us/sample - loss: 0.2860 - accuracy: 0.9636 - val_loss: 4.2328 - val_accuracy: 0.9192\n",
      "Epoch 187/800\n",
      "3352/3352 [==============================] - 1s 155us/sample - loss: 0.2638 - accuracy: 0.9597 - val_loss: 4.0826 - val_accuracy: 0.9234\n",
      "Epoch 188/800\n",
      "3352/3352 [==============================] - 0s 137us/sample - loss: 0.2591 - accuracy: 0.9609 - val_loss: 4.1954 - val_accuracy: 0.9220\n",
      "Epoch 189/800\n",
      "3352/3352 [==============================] - 0s 117us/sample - loss: 0.2818 - accuracy: 0.9636 - val_loss: 4.2813 - val_accuracy: 0.9262\n",
      "Epoch 190/800\n",
      "3352/3352 [==============================] - 1s 154us/sample - loss: 0.2423 - accuracy: 0.9648 - val_loss: 4.5113 - val_accuracy: 0.9206\n",
      "Epoch 191/800\n",
      "3352/3352 [==============================] - 0s 116us/sample - loss: 0.3785 - accuracy: 0.9588 - val_loss: 4.2689 - val_accuracy: 0.9234\n",
      "Epoch 192/800\n",
      "3352/3352 [==============================] - 0s 130us/sample - loss: 0.2386 - accuracy: 0.9609 - val_loss: 4.3243 - val_accuracy: 0.9164\n",
      "Epoch 193/800\n",
      "3352/3352 [==============================] - 1s 150us/sample - loss: 0.2507 - accuracy: 0.9636 - val_loss: 4.2507 - val_accuracy: 0.9206\n",
      "Epoch 194/800\n",
      "3352/3352 [==============================] - 0s 127us/sample - loss: 0.3398 - accuracy: 0.9621 - val_loss: 4.1972 - val_accuracy: 0.9178\n",
      "Epoch 195/800\n",
      "3352/3352 [==============================] - 0s 117us/sample - loss: 0.3075 - accuracy: 0.9567 - val_loss: 4.0720 - val_accuracy: 0.9206\n",
      "Epoch 196/800\n",
      "3352/3352 [==============================] - 1s 157us/sample - loss: 0.2960 - accuracy: 0.9591 - val_loss: 4.0438 - val_accuracy: 0.9248\n",
      "Epoch 197/800\n",
      "3352/3352 [==============================] - 0s 119us/sample - loss: 0.2612 - accuracy: 0.9678 - val_loss: 4.1629 - val_accuracy: 0.9276\n",
      "Epoch 198/800\n",
      "3352/3352 [==============================] - 0s 134us/sample - loss: 0.3111 - accuracy: 0.9615 - val_loss: 4.2450 - val_accuracy: 0.9220\n",
      "Epoch 199/800\n",
      "3352/3352 [==============================] - 1s 166us/sample - loss: 0.3494 - accuracy: 0.9588 - val_loss: 4.2541 - val_accuracy: 0.9220\n",
      "Epoch 200/800\n",
      "3352/3352 [==============================] - 0s 123us/sample - loss: 0.2169 - accuracy: 0.9636 - val_loss: 4.2326 - val_accuracy: 0.9304\n",
      "Epoch 201/800\n",
      "3352/3352 [==============================] - 0s 143us/sample - loss: 0.2122 - accuracy: 0.9666 - val_loss: 4.2454 - val_accuracy: 0.9276\n",
      "Epoch 202/800\n",
      "3352/3352 [==============================] - 0s 126us/sample - loss: 0.2720 - accuracy: 0.9657 - val_loss: 4.1892 - val_accuracy: 0.9276\n",
      "Epoch 203/800\n",
      "3352/3352 [==============================] - 0s 130us/sample - loss: 0.2267 - accuracy: 0.9642 - val_loss: 4.3698 - val_accuracy: 0.9248\n",
      "Epoch 204/800\n",
      "3352/3352 [==============================] - 0s 131us/sample - loss: 0.2033 - accuracy: 0.9678 - val_loss: 4.3431 - val_accuracy: 0.9262\n",
      "Epoch 205/800\n",
      "3352/3352 [==============================] - 0s 136us/sample - loss: 0.2438 - accuracy: 0.9645 - val_loss: 4.4040 - val_accuracy: 0.9234\n",
      "Epoch 206/800\n",
      "3352/3352 [==============================] - 0s 126us/sample - loss: 0.2523 - accuracy: 0.9645 - val_loss: 4.6418 - val_accuracy: 0.9318\n",
      "Epoch 207/800\n",
      "3352/3352 [==============================] - 0s 142us/sample - loss: 0.2044 - accuracy: 0.9696 - val_loss: 4.5338 - val_accuracy: 0.9290\n",
      "Epoch 208/800\n",
      "3352/3352 [==============================] - 0s 128us/sample - loss: 0.2581 - accuracy: 0.9663 - val_loss: 4.2808 - val_accuracy: 0.9248\n",
      "Epoch 209/800\n",
      "3352/3352 [==============================] - 0s 142us/sample - loss: 0.2169 - accuracy: 0.9687 - val_loss: 4.2592 - val_accuracy: 0.9262\n",
      "Epoch 210/800\n",
      "3352/3352 [==============================] - 1s 179us/sample - loss: 0.2146 - accuracy: 0.9621 - val_loss: 4.4139 - val_accuracy: 0.9248\n",
      "Epoch 211/800\n",
      "3352/3352 [==============================] - 0s 121us/sample - loss: 0.2677 - accuracy: 0.9672 - val_loss: 4.1401 - val_accuracy: 0.9276\n",
      "Epoch 212/800\n",
      "3352/3352 [==============================] - 0s 117us/sample - loss: 0.2370 - accuracy: 0.9627 - val_loss: 4.2096 - val_accuracy: 0.9262\n",
      "Epoch 213/800\n",
      "3352/3352 [==============================] - 1s 158us/sample - loss: 0.1959 - accuracy: 0.9723 - val_loss: 4.5922 - val_accuracy: 0.9290\n",
      "Epoch 214/800\n",
      "3352/3352 [==============================] - 0s 125us/sample - loss: 0.1872 - accuracy: 0.9714 - val_loss: 4.4971 - val_accuracy: 0.9276\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid, y_valid, valid_sample_weights),\n",
    "                    callbacks= [early_stopping_cb, onecycle], class_weight = my_class_weights,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_classes(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9343675417661098"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3052,   62,   80,   39],\n",
       "       [   1,   21,   12,    6],\n",
       "       [   0,    3,   27,    9],\n",
       "       [   0,    0,    8,   32]], dtype=int64)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "con_mat = confusion_matrix(y_train, y_train_pred)\n",
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7403306347997811"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_per_class_accuracy(con_mat):\n",
    "    sum_total = 0\n",
    "    num_of_classes = con_mat.shape[0]\n",
    "    for i in range (0, num_of_classes):\n",
    "        sum_total += con_mat[i,i]/con_mat[i].sum()\n",
    "    #con_mat[3,3]/con_mat[3].sum() + con_mat[2,2]/con_mat[2].sum() + con_mat[1,1]/con_mat[1].sum()\n",
    "    mean_per_class_accuracy = sum_total / num_of_classes\n",
    "    return mean_per_class_accuracy\n",
    "\n",
    "mean_per_class_accuracy(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9164345403899722"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_pred = model.predict_classes(X_valid)\n",
    "\n",
    "accuracy_score(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[644,  23,  17,   9],\n",
       "       [  1,   3,   3,   2],\n",
       "       [  0,   0,   5,   3],\n",
       "       [  0,   2,   0,   6]], dtype=int64)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_mat = confusion_matrix(y_valid, y_valid_pred)\n",
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6594065656565656"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_per_class_accuracy(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model based on TF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_valid_steps = (full_dataset_size * 0.15) // my_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "history = model.fit(train_dataset, epochs=n_epochs,\n",
    "                    validation_data= valid_dataset, validation_steps = my_valid_steps, \n",
    "                    callbacks= [early_stopping_cb], class_weight = my_class_weight,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Ordinal loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_cat = keras.utils.to_categorical(y_valid, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "def ordinal_loss(y_true, y_pred):\n",
    "    weights = K.cast(K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n",
    "    return (1.0 + weights) * losses.categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=ordinal_loss,\n",
    "              optimizer=my_optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "history = model.fit(X_train, y_train_cat, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid, y_valid_cat),\n",
    "                    callbacks= [early_stopping_cb], class_weight = my_class_weights,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_classes(X_train)\n",
    "\n",
    "accuracy_score(np.argmax(y_train_cat, axis= 1), y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(np.argmax(y_train_cat, axis= 1), y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = model.predict_classes(X_valid)\n",
    "\n",
    "accuracy_score(np.argmax(y_valid_cat, axis= 1), y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mat = confusion_matrix(np.argmax(y_valid_cat, axis= 1), y_valid_pred)\n",
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_per_class_accuracy(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=my_optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mc = np.argmax((np.round(np.mean([mc_model.predict(X_valid) for sample in range(100)], axis=0), 2)), axis= 1)\n",
    "# rounding (as above) seems to perform better\n",
    "#y_mc = np.argmax((np.mean([mc_model.predict(X_valid) for sample in range(100)], axis=0)), axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9401114206128134"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, y_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[660,  17,  10,   6],\n",
       "       [  1,   3,   3,   2],\n",
       "       [  0,   1,   6,   1],\n",
       "       [  1,   1,   0,   6]], dtype=int64)"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_mat = confusion_matrix(y_valid, y_mc)\n",
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6964285714285714"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_per_class_accuracy(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_topk = (np.round(np.mean([mc_model.predict(X_valid) for sample in range(100)], axis=0), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96796656"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_2_accuracy(y_true, y_pred):\n",
    "    top2_accuracy = tf.keras.metrics.TopKCategoricalAccuracy(2)\n",
    "    top2_accuracy.update_state(keras.utils.to_categorical(y_true, num_classes=4), y_pred)\n",
    "    return top2_accuracy.result().numpy()\n",
    "    \n",
    "top_2_accuracy(y_valid, y_topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(X_test)\n",
    "X_test = pd.DataFrame(X, columns=X_test.columns,\n",
    "                          index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = (X_test - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_mc_test = np.argmax((np.round(np.mean([mc_model.predict(X_test) for sample in range(100)], axis=0), 2)), axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9346314325452016"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[654,  15,  17,   7],\n",
       "       [  1,   5,   0,   2],\n",
       "       [  0,   0,   5,   4],\n",
       "       [  0,   0,   1,   8]], dtype=int64)"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_mat = confusion_matrix(y_test, y_test_predicted)\n",
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.753291847041847"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_per_class_accuracy(con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9735744"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_topk = (np.round(np.mean([mc_model.predict(X_test) for sample in range(100)], axis=0), 2))\n",
    "\n",
    "top_2_accuracy(y_test, y_topk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
